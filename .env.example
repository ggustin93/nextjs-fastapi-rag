# =============================================================================
# nextjs-fastapi-rag Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your environment.
# All settings have sensible defaults - only DATABASE_URL and API keys required.

# =============================================================================
# REQUIRED SETTINGS
# =============================================================================

# Database Configuration (Required)
# PostgreSQL with pgvector extension for vector similarity search
DATABASE_URL=postgresql://raguser:ragpass123@localhost:5432/postgres

# API Key (Required - unless using local models via custom base URL)
# Supports OpenAI, Chutes.ai, or any OpenAI-compatible API
# Get OpenAI key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-api-key-here

# =============================================================================
# LLM CONFIGURATION
# =============================================================================
# Supports OpenAI, Chutes.ai (Bittensor), Ollama, and any OpenAI-compatible API

# LLM Provider identifier (used for PydanticAI model string)
# Examples: "openai", "anthropic", "ollama"
# Default: openai
LLM_PROVIDER=openai

# LLM Model name
# OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo
# Chutes.ai: meta-llama/Llama-3.1-8B-Instruct, etc.
# Ollama: llama3, mistral, etc.
# Default: gpt-4o-mini
LLM_MODEL=gpt-4o-mini

# Custom API base URL for OpenAI-compatible APIs (optional)
# Set this to use Chutes.ai, Ollama, or other providers
# Examples:
#   Chutes.ai: https://myuser-my-chute.chutes.ai/v1
#   Ollama: http://localhost:11434/v1
# Leave unset for standard OpenAI API
# LLM_BASE_URL=

# API key for LLM provider (falls back to OPENAI_API_KEY if not set)
# LLM_API_KEY=

# Legacy setting (deprecated, use LLM_MODEL instead)
# LLM_CHOICE=gpt-4o-mini

# =============================================================================
# RAG AGENT CONFIGURATION
# =============================================================================
# Customize the RAG agent's behavior and response style

# System prompt for the RAG agent (optional)
# Override the default prompt to customize agent behavior for specific domains
# Default: Generic knowledge assistant prompt in French
# Use cases: Legal assistant, medical advisor, technical support, etc.
#
# Example - Legal domain:
# RAG_SYSTEM_PROMPT="Tu es un expert juridique belge avec accès à une base de textes légaux..."
#
# Example - Medical domain:
# RAG_SYSTEM_PROMPT="Tu es un assistant médical avec accès à des protocoles médicaux..."
#
# RAG_SYSTEM_PROMPT=

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================
# Embeddings can use a different provider than the main LLM

# Embedding model name
# OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# Default: text-embedding-3-small
EMBEDDING_MODEL=text-embedding-3-small

# Custom API base URL for embeddings (optional)
# Set this to use Chutes.ai or other providers for embeddings
# Example: https://myuser-my-chute.chutes.ai/v1
# EMBEDDING_BASE_URL=

# API key for embeddings (falls back to LLM_API_KEY, then OPENAI_API_KEY)
# EMBEDDING_API_KEY=

# Batch size for embedding generation
# Higher = faster but more memory, lower = slower but memory efficient
# Default: 100
EMBEDDING_BATCH_SIZE=100

# Retry settings for API calls
# Default: 3 retries with 1.0s initial delay (exponential backoff)
EMBEDDING_MAX_RETRIES=3
EMBEDDING_RETRY_DELAY=1.0

# In-memory cache size for embeddings (number of entries)
# Default: 1000
EMBEDDING_CACHE_MAX_SIZE=1000

# Tokenizer model for accurate token counting during chunking
# Default: sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_TOKENIZER_MODEL=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# DATABASE POOL CONFIGURATION
# =============================================================================
# Connection pool settings for asyncpg

# Minimum pool connections (kept open even when idle)
# Default: 1
DB_POOL_MIN_SIZE=1

# Maximum pool connections
# Default: 5
DB_POOL_MAX_SIZE=5

# Command timeout in seconds
# Default: 60
DB_COMMAND_TIMEOUT=60

# Connection timeout in seconds
# Default: 30
DB_CONNECTION_TIMEOUT=30

# =============================================================================
# CHUNKING CONFIGURATION
# =============================================================================
# Document chunking settings for ingestion pipeline

# Target chunk size in characters
# Default: 1000
CHUNK_SIZE=1000

# Overlap between chunks in characters (for context continuity)
# Default: 200
CHUNK_OVERLAP=200

# Maximum chunk size (hard limit)
# Default: 2000
CHUNK_MAX_SIZE=2000

# Minimum chunk size (chunks smaller than this are merged)
# Default: 100
CHUNK_MIN_SIZE=100

# Maximum tokens per chunk (for embedding model limits)
# Default: 512
CHUNK_MAX_TOKENS=512

# =============================================================================
# SEARCH CONFIGURATION
# =============================================================================
# RAG search and retrieval settings

# Default number of results to return
# Default: 10
SEARCH_DEFAULT_LIMIT=10

# Maximum allowed results per query
# Default: 50
SEARCH_MAX_LIMIT=50

# Minimum similarity score for results (0.0-1.0)
# Results below this threshold are filtered out in PostgreSQL (efficient)
# Higher = stricter filtering, fewer but more relevant results
# Lower values (0.15-0.25) capture more context, rely on reranker for precision
# Default: 0.20
SEARCH_SIMILARITY_THRESHOLD=0.20

# Enable/disable FlashRank reranking
# Set to false to use raw hybrid search results (useful for debugging)
# Default: true
RERANK_ENABLED=true

# Enable/disable query reformulation
# Set to false to use original user query (useful for debugging)
# Default: true
REFORMULATE_QUERY_ENABLED=true

# =============================================================================
# API SERVER CONFIGURATION
# =============================================================================

# Server host
# Default: 0.0.0.0
API_HOST=0.0.0.0

# Server port
# Default: 8000
API_PORT=8000

# Slow request logging threshold in milliseconds
# Requests slower than this are logged as warnings
# Default: 500
SLOW_REQUEST_THRESHOLD_MS=500

# CORS allowed origins (comma-separated)
# Default: http://localhost:3000,http://localhost:3001,http://127.0.0.1:3000
CORS_ORIGINS=http://localhost:3000,http://localhost:3001,http://127.0.0.1:3000

# =============================================================================
# OSIRIS BRUSSELS API CONFIGURATION
# =============================================================================
# Enable worksite information retrieval from OSIRIS Brussels
# Requires valid OSIRIS API credentials

# OSIRIS API base URL
# Default: https://api.osiris.brussels
OSIRIS_BASE_URL=https://api.osiris.brussels

# OSIRIS API key (Base64-encoded Basic auth credentials)
# Format: echo -n "username:password" | base64
# Required for fetch_osiris_worksite tool to work
OSIRIS_API_KEY=your-base64-encoded-credentials

# Request timeout in seconds
# Default: 30
OSIRIS_TIMEOUT=30

# Maximum retry attempts
# Default: 3
OSIRIS_MAX_RETRIES=3

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Debug mode (enables additional logging and stack traces)
DEBUG_MODE=false

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# --- Standard OpenAI Setup ---
# OPENAI_API_KEY=sk-xxx
# LLM_MODEL=gpt-4o-mini
# EMBEDDING_MODEL=text-embedding-3-small

# --- Chutes.ai (Bittensor) Setup ---
# Uses OpenAI-compatible API with custom base URL
# LLM_BASE_URL=https://myuser-my-chute.chutes.ai/v1
# LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
# LLM_API_KEY=your-chutes-api-key
# EMBEDDING_BASE_URL=https://myuser-my-chute.chutes.ai/v1
# EMBEDDING_MODEL=your-embedding-model
# EMBEDDING_API_KEY=your-chutes-api-key

# --- Ollama (Local) Setup ---
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_MODEL=llama3
# LLM_API_KEY=not-needed
# Note: Ollama embeddings require separate configuration

# --- Mixed Provider Setup ---
# Use Chutes.ai for LLM, OpenAI for embeddings
# LLM_BASE_URL=https://myuser-my-chute.chutes.ai/v1
# LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
# LLM_API_KEY=your-chutes-api-key
# OPENAI_API_KEY=sk-xxx  # Used for embeddings
# EMBEDDING_MODEL=text-embedding-3-small
