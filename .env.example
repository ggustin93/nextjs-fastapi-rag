# =============================================================================
# nextjs-fastapi-rag Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your environment.
#
# QUICK START:
# 1. cp .env.example .env
# 2. Configure Supabase (SUPABASE_URL + SUPABASE_SERVICE_KEY)
# 3. Add OpenAI API key (OPENAI_API_KEY)
# 4. For ingestion/CLI, also set DATABASE_URL
#
# IMPORTANT: Do NOT use inline comments in .env files!
# ❌ WRONG:  OPENAI_API_KEY=sk-abc123  # My key
# ✅ CORRECT:
#            # My OpenAI key
#            OPENAI_API_KEY=sk-abc123
#
# All other settings have sensible defaults.

# =============================================================================
# REQUIRED SETTINGS
# =============================================================================

# Supabase Configuration (Required for REST API mode - used by FastAPI backend)
# Get from: https://app.supabase.com/project/_/settings/api
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key

# Direct PostgreSQL (Required for ingestion pipeline and CLI)
# Get from: Supabase Dashboard > Database Settings > Connection string
# Or use local PostgreSQL for development
DATABASE_URL=postgresql://postgres:password@db.your-project.supabase.co:5432/postgres?sslmode=require

# OpenAI API Key (Required - unless using local models)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-api-key-here

# =============================================================================
# LLM CONFIGURATION
# =============================================================================
# Supports OpenAI, Chutes.ai, Ollama, and any OpenAI-compatible API
# See docs/CHUTES_AI_INTEGRATION.md for multi-provider setup guide

# --- OPTION 1: OpenAI (Default) ---
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
# LLM_BASE_URL=https://api.openai.com/v1
# Falls back to OPENAI_API_KEY if not set
# LLM_API_KEY=

# --- OPTION 2: Chutes.ai via OpenRouter (Phase 1 - Recommended) ---
# Cost savings: 30-70% vs OpenAI, excellent RAG quality
# GLM-4.6: 128K context, hybrid thinking, tool support
# LLM_PROVIDER=openai
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_MODEL=chutes/glm-4.6
# Replace with your actual OpenRouter API key
# LLM_API_KEY=sk-or-your-openrouter-key-here

# Alternative Chutes.ai models:
# LLM_MODEL=chutes/mai-ds-r1        # 200K context, 200-300 tool calls support
# LLM_MODEL=chutes/deepseek-v3.1    # 671B params, efficient inference
# LLM_MODEL=chutes/qwen3            # Ultra-long input stability
# LLM_MODEL=mistralai/mistral-7b-instruct  # Fast, cost-effective

# --- OPTION 3: Self-Hosted (Phase 2 - Maximum Savings) ---
# LLM_PROVIDER=openai
# LLM_BASE_URL=http://localhost:11434/v1  # Ollama
# LLM_MODEL=mistral  # or llama3, qwen, deepseek

# =============================================================================
# RAG AGENT CONFIGURATION
# =============================================================================

# Enabled tools - JSON array (default: all tools enabled)
# '["weather"]' = search + weather, '[]' = search only
# ENABLED_TOOLS=

# =============================================================================
# DOMAIN CUSTOMIZATION (File-Based Configuration)
# =============================================================================
# This RAG system is DOMAIN-AGNOSTIC - customize for your organization!
#
# Configure via files (recommended) or environment variables:
#
# 1. SYSTEM PROMPT - Agent behavior and instructions
#    File: config/prompts/system_prompt.txt (default)
#    Env override: RAG_SYSTEM_PROMPT_FILE=/custom/path/prompt.txt
#    Inline override: RAG_SYSTEM_PROMPT="Your full prompt here..."
#
# 2. QUERY EXPANSION PROMPT - Domain-specific reformulation
#    File: config/prompts/query_expansion.txt (default)
#    Env override: QUERY_EXPANSION_PROMPT_FILE=/custom/path/expansion.txt
#
# 3. STOPWORDS - Language-specific filtering
#    File: config/stopwords.json (default)
#    Format: {"default": [...], "fr": [...], "en": [...]}
#    Env override: STOPWORDS_FILE=/custom/path/stopwords.json

# --- System Prompt Options ---
# RAG_SYSTEM_PROMPT_FILE=config/prompts/system_prompt.txt

# --- Query Expansion ---
# Enable/disable query expansion via LLM (default: true)
QUERY_EXPANSION_ENABLED=true
# Model for query expansion (default: gpt-4o-mini - fast & cheap)
QUERY_EXPANSION_MODEL=gpt-4o-mini
# Custom prompt file for domain-specific terminology
# QUERY_EXPANSION_PROMPT_FILE=config/prompts/query_expansion.txt

# --- Title Re-Ranking ---
# Enable/disable title-based re-ranking (default: true)
TITLE_RERANK_ENABLED=true
# Maximum boost factor for matching titles (default: 0.15)
TITLE_RERANK_BOOST=0.15
# Classification patterns to extract from queries (comma-separated)
# Customize for your domain: "type" for Type A/B/C, "niveau" for Level 1/2/3, etc.
TITLE_RERANK_CLASSIFIERS=type,classe,categorie,niveau,phase,etape,version

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================
# See docs/CHUTES_AI_INTEGRATION.md for quality benchmarks and migration guide

# --- PHASE 1: OpenAI Embeddings (Recommended Start - 1536 dims) ---
# MTEB Score: ~62, proven production stability, no schema changes needed
EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small
# Falls back to LLM_API_KEY, then OPENAI_API_KEY if not set
# EMBEDDING_API_KEY=
EMBEDDING_BATCH_SIZE=100
EMBEDDING_MAX_RETRIES=3
EMBEDDING_RETRY_DELAY=1.0

# --- PHASE 2: E5 Embeddings (Best Quality - 1024 dims) ---
# MTEB Score: ~63 (BETTER than OpenAI!), 100% Top-5 accuracy, 2-3x faster
# ⚠️ REQUIRES: pgvector schema migration (1536→1024) + document re-ingestion
# Uncomment after completing migration steps in docs/CHUTES_AI_INTEGRATION.md:
# EMBEDDING_MODEL=intfloat/e5-large-v2
# Leave empty for local inference
# EMBEDDING_BASE_URL=
# Not needed for local models
# EMBEDDING_API_KEY=
# EMBEDDING_BATCH_SIZE=100
# EMBEDDING_MAX_RETRIES=3
# EMBEDDING_RETRY_DELAY=1.0

# Alternative open-source embeddings (all require schema changes):
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5  # 1024 dims, excellent multilingual
# EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2  # 768 dims, CPU-friendly

# Shared settings (apply to all embedding models)
EMBEDDING_CACHE_MAX_SIZE=1000
EMBEDDING_TOKENIZER_MODEL=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# SEARCH CONFIGURATION
# =============================================================================

# Default/max results per query (default: 30/100)
SEARCH_DEFAULT_LIMIT=30
SEARCH_MAX_LIMIT=100

# Similarity threshold for results (default: 0.30)
# Lower = more results but risk hallucination on irrelevant chunks
# Higher = stricter filtering, less noise
# WARNING: Values below 0.25 significantly increase hallucination risk
SEARCH_SIMILARITY_THRESHOLD=0.30

# Out-of-scope detection threshold (default: 0.45)
# If best result is below this, tool returns "HORS PÉRIMÈTRE" refusal
# Should be higher than SEARCH_SIMILARITY_THRESHOLD
# WARNING: Values below 0.40 may let LLM hallucinate on low-quality results
OUT_OF_SCOPE_THRESHOLD=0.45

# RRF k parameter for hybrid search ranking (default: 50)
RRF_K=50

# Exclude TOC chunks from search (default: true)
EXCLUDE_TOC=true

# =============================================================================
# CHUNKING CONFIGURATION
# =============================================================================

# Chunk size settings in characters
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNK_MAX_SIZE=2000
CHUNK_MIN_SIZE=100
CHUNK_MAX_TOKENS=512

# =============================================================================
# DATABASE POOL CONFIGURATION
# =============================================================================

# Connection pool settings for asyncpg (direct PostgreSQL mode)
DB_POOL_MIN_SIZE=1
DB_POOL_MAX_SIZE=5
DB_COMMAND_TIMEOUT=60
DB_CONNECTION_TIMEOUT=30

# =============================================================================
# API SERVER CONFIGURATION
# =============================================================================

API_HOST=0.0.0.0
API_PORT=8000
SLOW_REQUEST_THRESHOLD_MS=500
CORS_ORIGINS=http://localhost:3000,http://localhost:3001,http://127.0.0.1:3000

# =============================================================================
# WEATHER TOOL CONFIGURATION (Open-Meteo - no API key required)
# =============================================================================

# WEATHER_BASE_URL=https://api.open-meteo.com/v1/forecast
# WEATHER_GEOCODE_URL=https://geocoding-api.open-meteo.com/v1/search
# WEATHER_CACHE_TTL=900
# WEATHER_TIMEOUT=5
# WEATHER_TEMPERATURE_UNIT=celsius

# =============================================================================
# OSIRIS BRUSSELS WORKSITE TOOL CONFIGURATION
# =============================================================================

# OSIRIS API endpoint for Brussels worksites (GeoJSON format)
# OSIRIS_BASE_URL=https://api.osiris.brussels/geoserver/ogc/features/v1/collections/api:WORKSITES/items

# Basic authentication credentials
# OSIRIS_USERNAME=cdco
OSIRIS_PASSWORD=*******

# Cache and timeout settings
# OSIRIS_CACHE_TTL=900
# OSIRIS_TIMEOUT=10
