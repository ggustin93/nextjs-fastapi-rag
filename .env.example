# =============================================================================
# nextjs-fastapi-rag Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your environment.
#
# QUICK START:
# 1. cp .env.example .env
# 2. Configure Supabase (SUPABASE_URL + SUPABASE_SERVICE_KEY)
# 3. Add OpenAI API key (OPENAI_API_KEY)
# 4. For ingestion/CLI, also set DATABASE_URL
#
# All other settings have sensible defaults.

# =============================================================================
# REQUIRED SETTINGS
# =============================================================================

# Supabase Configuration (Required for REST API mode - used by FastAPI backend)
# Get from: https://app.supabase.com/project/_/settings/api
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key

# Direct PostgreSQL (Required for ingestion pipeline and CLI)
# Get from: Supabase Dashboard > Database Settings > Connection string
# Or use local PostgreSQL for development
DATABASE_URL=postgresql://postgres:password@db.your-project.supabase.co:5432/postgres?sslmode=require

# OpenAI API Key (Required - unless using local models)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-api-key-here

# =============================================================================
# LLM CONFIGURATION
# =============================================================================
# Supports OpenAI, Chutes.ai, Ollama, and any OpenAI-compatible API
# See docs/CHUTES_AI_INTEGRATION.md for multi-provider setup guide

# --- OPTION 1: OpenAI (Default) ---
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_API_KEY=  # Falls back to OPENAI_API_KEY

# --- OPTION 2: Chutes.ai via OpenRouter (Phase 1 - Recommended) ---
# Cost savings: 30-70% vs OpenAI, excellent RAG quality
# GLM-4.6: 128K context, hybrid thinking, tool support
# LLM_PROVIDER=openai
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_MODEL=chutes/glm-4.6
# LLM_API_KEY=sk-or-your-openrouter-key-here

# Alternative Chutes.ai models:
# LLM_MODEL=chutes/mai-ds-r1        # 200K context, 200-300 tool calls support
# LLM_MODEL=chutes/deepseek-v3.1    # 671B params, efficient inference
# LLM_MODEL=chutes/qwen3            # Ultra-long input stability
# LLM_MODEL=mistralai/mistral-7b-instruct  # Fast, cost-effective

# --- OPTION 3: Self-Hosted (Phase 2 - Maximum Savings) ---
# LLM_PROVIDER=openai
# LLM_BASE_URL=http://localhost:11434/v1  # Ollama
# LLM_MODEL=mistral  # or llama3, qwen, deepseek

# =============================================================================
# RAG AGENT CONFIGURATION
# =============================================================================

# System prompt for the RAG agent (optional - has French default)
# Override to customize agent behavior for specific domains
# RAG_SYSTEM_PROMPT="Tu es un expert juridique belge..."

# Enabled tools - JSON array (default: all tools enabled)
# '["weather"]' = search + weather, '[]' = search only
# ENABLED_TOOLS=

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================
# See docs/CHUTES_AI_INTEGRATION.md for quality benchmarks and migration guide

# --- PHASE 1: OpenAI Embeddings (Recommended Start - 1536 dims) ---
# MTEB Score: ~62, proven production stability, no schema changes needed
EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_API_KEY=  # Falls back to OPENAI_API_KEY
EMBEDDING_BATCH_SIZE=100
EMBEDDING_MAX_RETRIES=3
EMBEDDING_RETRY_DELAY=1.0

# --- PHASE 2: E5 Embeddings (Best Quality - 1024 dims) ---
# MTEB Score: ~63 (BETTER than OpenAI!), 100% Top-5 accuracy, 2-3x faster
# ⚠️ REQUIRES: pgvector schema migration (1536→1024) + document re-ingestion
# Uncomment after completing migration steps in docs/CHUTES_AI_INTEGRATION.md:
# EMBEDDING_MODEL=intfloat/e5-large-v2
# EMBEDDING_BASE_URL=  # Leave empty for local inference
# EMBEDDING_API_KEY=  # Not needed for local models
# EMBEDDING_BATCH_SIZE=100
# EMBEDDING_MAX_RETRIES=3
# EMBEDDING_RETRY_DELAY=1.0

# Alternative open-source embeddings (all require schema changes):
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5  # 1024 dims, excellent multilingual
# EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2  # 768 dims, CPU-friendly

# Shared settings (apply to all embedding models)
EMBEDDING_CACHE_MAX_SIZE=1000
EMBEDDING_TOKENIZER_MODEL=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# SEARCH CONFIGURATION
# =============================================================================

# Default/max results per query (default: 30/100)
SEARCH_DEFAULT_LIMIT=30
SEARCH_MAX_LIMIT=100

# Similarity threshold for results (default: 0.25)
# Lower = more results, higher = stricter filtering
SEARCH_SIMILARITY_THRESHOLD=0.25

# Out-of-scope detection threshold (default: 0.40)
# If best result is below this, question is likely outside KB scope
OUT_OF_SCOPE_THRESHOLD=0.40

# RRF k parameter for hybrid search ranking (default: 50)
RRF_K=50

# Exclude TOC chunks from search (default: true)
EXCLUDE_TOC=true

# =============================================================================
# CHUNKING CONFIGURATION
# =============================================================================

# Chunk size settings in characters
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNK_MAX_SIZE=2000
CHUNK_MIN_SIZE=100
CHUNK_MAX_TOKENS=512

# =============================================================================
# DATABASE POOL CONFIGURATION
# =============================================================================

# Connection pool settings for asyncpg (direct PostgreSQL mode)
DB_POOL_MIN_SIZE=1
DB_POOL_MAX_SIZE=5
DB_COMMAND_TIMEOUT=60
DB_CONNECTION_TIMEOUT=30

# =============================================================================
# API SERVER CONFIGURATION
# =============================================================================

API_HOST=0.0.0.0
API_PORT=8000
SLOW_REQUEST_THRESHOLD_MS=500
CORS_ORIGINS=http://localhost:3000,http://localhost:3001,http://127.0.0.1:3000

# =============================================================================
# WEATHER TOOL CONFIGURATION (Open-Meteo - no API key required)
# =============================================================================

# WEATHER_BASE_URL=https://api.open-meteo.com/v1/forecast
# WEATHER_GEOCODE_URL=https://geocoding-api.open-meteo.com/v1/search
# WEATHER_CACHE_TTL=900
# WEATHER_TIMEOUT=5
# WEATHER_TEMPERATURE_UNIT=celsius
